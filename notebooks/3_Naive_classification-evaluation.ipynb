{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1653094647964,
     "user": {
      "displayName": "Guillermo Alcantara Gonzalez",
      "userId": "12123596696743696964"
     },
     "user_tz": 300
    },
    "id": "2RXruDQ1zY8V"
   },
   "outputs": [],
   "source": [
    "#@title Read Data\n",
    "import pandas as pd\n",
    "path=\"/content/drive/MyDrive/wdir/growth-hacking-sentiment/\"\n",
    "df = pd.read_csv(path+\"data/raw/review_corpus.tsv\", sep=\"\\t\")\n",
    "\n",
    "ratings = list(df[\"rating\"])\n",
    "reviews = list(df[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19769,
     "status": "ok",
     "timestamp": 1653094667988,
     "user": {
      "displayName": "Guillermo Alcantara Gonzalez",
      "userId": "12123596696743696964"
     },
     "user_tz": 300
    },
    "id": "UEqSw8sh02oZ"
   },
   "outputs": [],
   "source": [
    "#@title Dictionary based sentiment analysis\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "positive_wds = set(opinion_lexicon.positive())\n",
    "negative_wds = set(opinion_lexicon.negative())\n",
    "# lists are NOT lemmatized so we only have to tokenize the text and count\n",
    "# positive and negative words\n",
    "\n",
    "\n",
    "def score_sent(sent):\n",
    "    \"\"\"Returns a score btw -1 and 1\"\"\"\n",
    "    sent = [e.lower() for e in sent if e.isalnum()]\n",
    "    total = len(sent)\n",
    "    pos = len([e for e in sent if e in positive_wds])\n",
    "    neg = len([e for e in sent if e in negative_wds])\n",
    "    if total > 0:\n",
    "        return (pos - neg) / total\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def score_review(review):\n",
    "    sentiment_scores = []\n",
    "    sents = sent_tokenize(review)\n",
    "    for sent in sents:\n",
    "        wds = word_tokenize(sent)\n",
    "        sent_scores = score_sent(wds)\n",
    "        sentiment_scores.append(sent_scores)\n",
    "    return sum(sentiment_scores) / len(sentiment_scores)\n",
    "\n",
    "\n",
    "review_sentiments = [score_review(e) for e in reviews]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"rating\": ratings,\n",
    "        \"review\": reviews,\n",
    "        \"review dictionary based sentiment\": review_sentiments,\n",
    "    }\n",
    ")\n",
    "    \n",
    "with open(path+\"data/processed/dictionary_based_sentiment.tsv\", \"w\") as outfile:\n",
    "    outfile.write(df.to_csv(index=False, sep=\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 1301,
     "status": "ok",
     "timestamp": 1653094669287,
     "user": {
      "displayName": "Guillermo Alcantara Gonzalez",
      "userId": "12123596696743696964"
     },
     "user_tz": 300
    },
    "id": "FJxm2908_TbD",
    "outputId": "918c71fc-9c64-4aa0-b011-937ac7415f8d"
   },
   "outputs": [],
   "source": [
    "#@title Exploratory Data Analysis\n",
    "# plot score vs dict_sents\n",
    "from collections import Counter\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# let's see the distributions\n",
    "\n",
    "# the distribution of review scores\n",
    "rating_counts = Counter(ratings)\n",
    "data1 = pd.DataFrame(\n",
    "    {\n",
    "        \"ratings\": [str(e) for e in list(rating_counts.keys())],\n",
    "        \"counts\": list(rating_counts.values()),\n",
    "    }\n",
    ")\n",
    "\n",
    "chart1 = alt.Chart(data1).mark_bar().encode(x=\"ratings\", y=\"counts\")\n",
    "chart1.save(f\"{path}plots/01/rating_counts.html\")\n",
    "# we have a majority class !\n",
    "\n",
    "# the distribution of sentiment scores\n",
    "hist, bin_edges = np.histogram(review_sentiments, density=True)\n",
    "labels = list(zip(bin_edges, bin_edges[1:]))\n",
    "labels = [(str(e[0]), str(e[1])) for e in labels]\n",
    "labels = [\" \".join(e) for e in labels]\n",
    "\n",
    "\n",
    "data2 = pd.DataFrame({\"sentiment scores\": labels, \"counts\": hist})\n",
    "\n",
    "chart2 = (\n",
    "    alt.Chart(data2)\n",
    "    .mark_bar()\n",
    "    .encode(x=alt.X(\"sentiment scores\", sort=labels), y=\"counts\")\n",
    ")\n",
    "chart2.save(f\"{path}plots/01/review_sentiments.html\")\n",
    "# (0.0, 0.20000000000000018) -> neutral is the majority\n",
    "\n",
    "\n",
    "# is there any relationship btw review scores and sentiments?\n",
    "source = pd.DataFrame(\n",
    "    {\"ratings\": [str(e) for e in ratings], \"sentiments\": review_sentiments}\n",
    ")\n",
    "\n",
    "\n",
    "chart4 = (\n",
    "    alt.Chart(source)\n",
    "    .mark_circle(size=60)\n",
    "    .encode(\n",
    "        x=\"ratings\", y=\"sentiments\", color=\"ratings\", tooltip=[\"ratings\", \"sentiments\"]\n",
    "    )\n",
    "    .interactive()\n",
    ")\n",
    "\n",
    "chart4.save(f\"{path}plots/01/reviews_ratings_vs_sentiment.html\")\n",
    "chart4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1653094669287,
     "user": {
      "displayName": "Guillermo Alcantara Gonzalez",
      "userId": "12123596696743696964"
     },
     "user_tz": 300
    },
    "id": "1LZnUlFR5udj",
    "outputId": "2c05b726-f944-46db-8af2-bb0c17fb40eb"
   },
   "outputs": [],
   "source": [
    "#@title Correlation\n",
    "# test correlation\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "corr1, _ = pearsonr(ratings, review_sentiments)\n",
    "print(corr1)\n",
    "\n",
    "# Spearman rank correlation says there's weak correlation btw review score\n",
    "# and sentiment\n",
    "scor1, _ = spearmanr(ratings, review_sentiments)\n",
    "\n",
    "print(scor1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dz-6VE2M5qS-"
   },
   "source": [
    "ok, we plotted to see the distribution, but it's not normal, so it can be omitted on pearson because it assumes a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVP4-LLL5rdL"
   },
   "source": [
    "Verbal negations have a big impact on the meaning of a world or a phrase. Let's mark them.\n",
    "\n",
    "- no issues\n",
    "- no complains\n",
    "- Doesn't work.\n",
    "- Didn't like it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1653094669288,
     "user": {
      "displayName": "Guillermo Alcantara Gonzalez",
      "userId": "12123596696743696964"
     },
     "user_tz": 300
    },
    "id": "9-tgv1Ot6LcC",
    "outputId": "fc0edf2c-4d85-4699-cca1-6a6e6664e28b"
   },
   "outputs": [],
   "source": [
    "# Reference: https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php\n",
    "#@markdown Let's see  the data\n",
    "\n",
    "for i in range(len(reviews)):\n",
    "    sc = ratings[i]\n",
    "    rs = review_sentiments[i]\n",
    "    # ss = summary_sentiments[i]\n",
    "    t = reviews[i]\n",
    "    if sc == 5 and rs < -0.2:\n",
    "        print(t)\n",
    "    if sc == 1 and rs > 0.3:\n",
    "        print(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1653094669288,
     "user": {
      "displayName": "Guillermo Alcantara Gonzalez",
      "userId": "12123596696743696964"
     },
     "user_tz": 300
    },
    "id": "P2coVkCr08VK",
    "outputId": "f1d9fef1-c815-42e8-b48e-f4c58f96da0c"
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "\n",
    "t = \"I received these on time and no problems. No damages battlfield never fails\"\n",
    "print(mark_negation(t.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19472,
     "status": "ok",
     "timestamp": 1653094688747,
     "user": {
      "displayName": "Guillermo Alcantara Gonzalez",
      "userId": "12123596696743696964"
     },
     "user_tz": 300
    },
    "id": "Y3hlpNXn-fLk",
    "outputId": "ca491194-2751-4cc3-825a-61da63bf4496"
   },
   "outputs": [],
   "source": [
    "#@markdown Let's handle negation\n",
    "positive_wds = set(opinion_lexicon.positive())\n",
    "negative_wds = set(opinion_lexicon.negative())\n",
    "\n",
    "positive_wds_with_negation = positive_wds.union({wd + \"_NEG\" for wd in negative_wds})\n",
    "negative_wds_with_negation = negative_wds.union({wd + \"_NEG\" for wd in positive_wds})\n",
    "\n",
    "\n",
    "def score_sent(sent):\n",
    "    \"\"\"Returns a score btw -1 and 1\"\"\"\n",
    "    sent = [e.lower() for e in sent if e.isalnum()]\n",
    "    total = len(sent)\n",
    "    pos = len([e for e in sent if e in positive_wds_with_negation])\n",
    "    neg = len([e for e in sent if e in negative_wds_with_negation])\n",
    "    if total > 0:\n",
    "        return (pos - neg) / total\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def score_review(review):\n",
    "    sentiment_scores = []\n",
    "    sents = sent_tokenize(review)\n",
    "    for sent in sents:\n",
    "        wds = word_tokenize(sent)\n",
    "        wds = mark_negation(wds)\n",
    "        sent_scores = score_sent(wds)\n",
    "        sentiment_scores.append(sent_scores)\n",
    "    return sum(sentiment_scores) / len(sentiment_scores)\n",
    "\n",
    "\n",
    "review_sentiments = [score_review(e) for e in reviews]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\"rating\": ratings, \"review\": reviews, \"review sentiment\": review_sentiments,}\n",
    ")\n",
    "\n",
    "with open(f\"{path}data/processed/rule_based_sentiment.tsv\", \"w\") as outfile:\n",
    "    outfile.write(df.to_csv(index=False, sep=\"\\t\"))\n",
    "\n",
    "scor1, _ = spearmanr(ratings, review_sentiments)\n",
    "print(scor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1653094688748,
     "user": {
      "displayName": "Guillermo Alcantara Gonzalez",
      "userId": "12123596696743696964"
     },
     "user_tz": 300
    },
    "id": "Qa84CoYxDmBo"
   },
   "outputs": [],
   "source": [
    "#@markdown Let's see the distributions the distribution of sentiment scores\n",
    "hist, bin_edges = np.histogram(review_sentiments, density=True)\n",
    "labels = list(zip(bin_edges, bin_edges[1:]))\n",
    "labels = [(str(e[0]), str(e[1])) for e in labels]\n",
    "labels = [\" \".join(e) for e in labels]\n",
    "\n",
    "\n",
    "data2 = pd.DataFrame({\"sentiment scores\": labels, \"counts\": hist})\n",
    "\n",
    "chart2 = (\n",
    "    alt.Chart(data2)\n",
    "    .mark_bar()\n",
    "    .encode(x=alt.X(\"sentiment scores\", sort=labels), y=\"counts\")\n",
    ")\n",
    "chart2.save(f\"{path}plots/02/review_sentiments.html\")\n",
    "# (0.0, 0.20000000000000018) -> neutral is the majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1653094689107,
     "user": {
      "displayName": "Guillermo Alcantara Gonzalez",
      "userId": "12123596696743696964"
     },
     "user_tz": 300
    },
    "id": "isRnefiUDnNo"
   },
   "outputs": [],
   "source": [
    "#@markdown Is there any relationship btw review scores and sentiments?\n",
    "source = pd.DataFrame(\n",
    "    {\"ratings\": [str(e) for e in ratings], \"sentiments\": review_sentiments}\n",
    ")\n",
    "\n",
    "chart4 = (\n",
    "    alt.Chart(source)\n",
    "    .mark_circle(size=60)\n",
    "    .encode(\n",
    "        x=\"ratings\", y=\"sentiments\", color=\"ratings\", tooltip=[\"ratings\", \"sentiments\"]\n",
    "    )\n",
    "    .interactive()\n",
    ")\n",
    "chart4.save(f\"{path}plots/02/reviews_raings_vs_sentiment.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GuSusK-MZQh"
   },
   "source": [
    "# Naive classification: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpaRPSFVMabO"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Corpus\n",
    "df = pd.read_csv(\"data/processed/sentiment_with_lemmas.tsv\", sep=\"\\t\")\n",
    "\n",
    "ratings = list(df[\"rating\"])\n",
    "reviews = list(df[\"review\"])\n",
    "reviews = [str(e) for e in reviews]\n",
    "lemmatized = list(df[\"lemmas\"])\n",
    "lemmatized = [str(e).split() for e in lemmatized]\n",
    "lemmatized = [[e[0] + \"_\" + e[1] for e in list(nltk.bigrams(e))] for e in lemmatized]\n",
    "sentiment = list(df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ce6Uidq4NnBy"
   },
   "outputs": [],
   "source": [
    "#@title Sentiment values instead of scores\n",
    "\n",
    "\n",
    "def get_rating_class(rating):\n",
    "    if rating > 4:\n",
    "        return \"positive\"\n",
    "    elif 2 <= rating <= 4:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "\n",
    "\n",
    "def get_sentiment_value(sentiment):\n",
    "    if sentiment > 0.2:\n",
    "        return \"positive\"\n",
    "    elif -0.2 <= sentiment <= 0.2:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "\n",
    "\n",
    "def check_status(e):\n",
    "    if e[0] == e[1]:\n",
    "        return \"OK\"\n",
    "    else:\n",
    "        return \"CHECK\"\n",
    "\n",
    "\n",
    "rating_classes = [get_rating_class(e) for e in ratings]\n",
    "sentiment_values = [get_sentiment_value(e) for e in sentiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gUmrzeYRdHl"
   },
   "outputs": [],
   "source": [
    "##@markdown  Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(rating_classes, sentiment_values)\n",
    "print(acc)\n",
    "# 0.4315555555555556\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "print(\n",
    "    classification_report(rating_classes, sentiment_values, target_names=target_names)\n",
    ")\n",
    "\n",
    "#                 precision    recall  f1-score   support\n",
    "#     negative       0.80      0.08      0.14      1500\n",
    "#      neutral       0.36      0.93      0.52      1500\n",
    "#     positive       0.82      0.29      0.43      1500\n",
    "#     accuracy                           0.43      4500\n",
    "#    macro avg       0.66      0.43      0.36      4500\n",
    "# weighted avg       0.66      0.43      0.36      4500\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "x, y = np.meshgrid(range(0, 3), range(0, 3))\n",
    "cm = confusion_matrix(rating_classes, sentiment_values, labels=[\"negative\",\n",
    "                                                                \"netural\",\n",
    "                                                                \"positive\"])\n",
    "\n",
    "source = pd.DataFrame({\"true\": x.ravel(), \"predicted\": y.ravel(), \"number\": cm.ravel()})\n",
    "\n",
    "chart = (\n",
    "    alt.Chart(source)\n",
    "    .mark_rect()\n",
    "    .encode(x=\"true:O\", y=\"predicted:O\", color=\"number:Q\", tooltip=[\"number\"])\n",
    "    .interactive()\n",
    "    .properties(width=800, height=500)\n",
    ")\n",
    "chart.save(\"plots/05/confusion_matrix.html\")\n",
    "\n",
    "acc = accuracy_score(rating_classes, sentiment_values)\n",
    "print(acc)\n",
    "# 0.4315555555555556\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "print(\n",
    "    classification_report(rating_classes, sentiment_values, target_names=target_names)\n",
    ")\n",
    "\n",
    "#                 precision    recall  f1-score   support\n",
    "#     negative       0.80      0.08      0.14      1500\n",
    "#      neutral       0.36      0.93      0.52      1500\n",
    "#     positive       0.82      0.29      0.43      1500\n",
    "#     accuracy                           0.43      4500\n",
    "#    macro avg       0.66      0.43      0.36      4500\n",
    "# weighted avg       0.66      0.43      0.36      4500\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "x, y = np.meshgrid(range(0, 3), range(0, 3))\n",
    "cm = confusion_matrix(rating_classes, sentiment_values, labels=[\"negative\",\n",
    "                                                                \"netural\",\n",
    "                                                                \"positive\"])\n",
    "\n",
    "source = pd.DataFrame({\"true\": x.ravel(), \"predicted\": y.ravel(), \"number\": cm.ravel()})\n",
    "\n",
    "chart = (\n",
    "    alt.Chart(source)\n",
    "    .mark_rect()\n",
    "    .encode(x=\"true:O\", y=\"predicted:O\", color=\"number:Q\", tooltip=[\"number\"])\n",
    "    .interactive()\n",
    "    .properties(width=800, height=500)\n",
    ")\n",
    "chart.save(\"plots/05/confusion_matrix.html\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPykr8zT+F2AkNBtvLm5s45",
   "collapsed_sections": [
    "y4a3P5nC23e6",
    "MW7heL1A2s_l",
    "mf2GusIF2xJR",
    "6_8gnlKN3DuA",
    "WM7nKDmvCmN1"
   ],
   "mount_file_id": "1RtuyhxoeoaS7_NFxyhw-a46atJGe-imq",
   "name": "Dictionary based classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "growth_hack:Python",
   "language": "python",
   "name": "conda-env-growth_hack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
