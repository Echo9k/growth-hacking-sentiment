{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyQRBB79ScMm"
   },
   "source": [
    "# Setting up the environment\n",
    "Don't forget to set the runtime to GPU.\n",
    "Mount your Google Drive. It'll be used to install the requirements and load the saved models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32157,
     "status": "ok",
     "timestamp": 1587902927833,
     "user": {
      "displayName": "Zoltán Varjú",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_2m2F3Q5ufS6p0I4a7oD7NgOzPTaq7hFklmEinQ=s64",
      "userId": "15747406381911019387"
     },
     "user_tz": -120
    },
    "id": "LjkUMpmsRgfN",
    "outputId": "022b6ce7-4d32-48f7-d3e5-986e07bf1c23"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVPGvBg6TKcW"
   },
   "source": [
    "Install requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16682,
     "status": "ok",
     "timestamp": 1587902947410,
     "user": {
      "displayName": "Zoltán Varjú",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_2m2F3Q5ufS6p0I4a7oD7NgOzPTaq7hFklmEinQ=s64",
      "userId": "15747406381911019387"
     },
     "user_tz": -120
    },
    "id": "pZS4OOSdSz6W",
    "outputId": "a248cb87-7f1e-4be5-b826-9a8a16288b68"
   },
   "outputs": [],
   "source": [
    "!pip install -r /content/drive/My\\ Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j8V3GSvEUNTq"
   },
   "outputs": [],
   "source": [
    "# install apex\n",
    "%%writefile setup.sh\n",
    "\n",
    "export CUDA_HOME=/usr/local/cuda-10.1\n",
    "git clone https://github.com/NVIDIA/apex\n",
    "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jV5c2eiuUYRM"
   },
   "outputs": [],
   "source": [
    "!sh setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J9uGdg3GXghy"
   },
   "source": [
    "# Loading our model and making predictions\n",
    "\n",
    "## Load your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "snngrVUeZXq1"
   },
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "\n",
    "model2 = ClassificationModel(\n",
    "    model_type=\"distilbert\",\n",
    "    model_name= \"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/outputs2/best_model/\",\n",
    "    use_cuda=True,\n",
    "    num_labels=3,\n",
    "    args={\n",
    "        \"output_dir\": \"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/outputs2/best_model/\",\n",
    "        \"reprocess_input_data\": True,\n",
    "        \"sliding_window\": True,\n",
    "        \"max_seq_length\": 512,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CCHzzRvh07W"
   },
   "source": [
    "Read in the reviews and classify them. WARNING! Classification takes some time, you can have a coffee or two before continouning this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NAuX1_iaAKk"
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "with open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/data/raw/reviews_without_ratings.txt\", \"r\") as f:\n",
    "    reviews = f.read().split(\"\\n\")\n",
    "\n",
    "# reviews = random.sample(reviews, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0M-Qqcoch6tF"
   },
   "outputs": [],
   "source": [
    "predictions = model2.predict(reviews)\n",
    "\n",
    "predicted_class, predicted_probas = predictions[0], predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CtdpsQcQCwdT"
   },
   "outputs": [],
   "source": [
    "# save the predictions, otherwise you have to run every sinppets above whenever you want to work with them\n",
    "import pickle\n",
    "with open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/predicted_class.pkl\", \"wb\") as outfile:\n",
    "  pickle.dump(predicted_class, outfile)\n",
    "\n",
    "with open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/predicted_probs.pkl\", \"wb\") as outfile:\n",
    "  pickle.dump(predicted_probas, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMmkcTgJjbEI"
   },
   "source": [
    "# Extracting keywords/key phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1363,
     "status": "ok",
     "timestamp": 1587903169833,
     "user": {
      "displayName": "Zoltán Varjú",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_2m2F3Q5ufS6p0I4a7oD7NgOzPTaq7hFklmEinQ=s64",
      "userId": "15747406381911019387"
     },
     "user_tz": -120
    },
    "id": "_w__E6oukDl8",
    "outputId": "5b5a60b6-82fe-4eab-e3a9-99206306e31c"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sm0N97VeZLU9"
   },
   "outputs": [],
   "source": [
    "# only if needed\n",
    "import pickle\n",
    "predicted_class = pickle.load(open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/predicted_class.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9i14_XzjlNU"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "blacklist = set(stopwords.words())\n",
    "\n",
    "\n",
    "def tokenize_review(review):\n",
    "    wds = []\n",
    "    for sent in sent_tokenize(review):\n",
    "        for wd in word_tokenize(sent):\n",
    "            if wd.lower() not in blacklist and wd.isalpha():\n",
    "                wds.append(wd.lower())\n",
    "    return wds\n",
    "\n",
    "\n",
    "reviews = [tokenize_review(review) for review in reviews]\n",
    "\n",
    "bigrams = [[e[0] + \"_\" + e[1] for e in list(nltk.bigrams(e))] for e in reviews]\n",
    "\n",
    "positive_reviews = [bigrams[i] for i in range(len(reviews)) if predicted_class[i] == 2]\n",
    "\n",
    "negative_reviews = [bigrams[i] for i in range(len(reviews)) if predicted_class[i] == 0]\n",
    "\n",
    "neutral_reviews = [bigrams[i] for i in range(len(reviews)) if predicted_class[i] == 1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1587903324178,
     "user": {
      "displayName": "Zoltán Varjú",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_2m2F3Q5ufS6p0I4a7oD7NgOzPTaq7hFklmEinQ=s64",
      "userId": "15747406381911019387"
     },
     "user_tz": -120
    },
    "id": "mfy0EtxelhjX",
    "outputId": "5dc09d48-daff-48ee-e919-4fd1a64d469e"
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"sentiments\": [\"negative\", \"neutral\", \"positive\"],\n",
    "                   \"counts\": [len(negative_reviews), len(neutral_reviews),\n",
    "                              len(positive_reviews)]})\n",
    "\n",
    "alt.Chart(df).mark_bar().encode(x=\"sentiments\", y=\"counts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYPeBW4dZpZL"
   },
   "outputs": [],
   "source": [
    "# save the review bigrams so you don't have to re-run the snippets above\n",
    "import pickle\n",
    "\n",
    "with open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/positive_reviews.pkl\", \"wb\") as outfile:\n",
    "  pickle.dump(positive_reviews, outfile)\n",
    "\n",
    "with open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/negative_reviews.pkl\", \"wb\") as outfile:\n",
    "  pickle.dump(negative_reviews, outfile)\n",
    "\n",
    "with open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/neutral_reviews.pkl\", \"wb\") as outfile:\n",
    "  pickle.dump(neutral_reviews, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpgkEdTLDVEg"
   },
   "source": [
    "Now, we can extract the key bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0je_gqpDUl7"
   },
   "outputs": [],
   "source": [
    "from keyness import log_likelihood\n",
    "\n",
    "positive_keys = log_likelihood(bigrams, positive_reviews)[:150]\n",
    "negative_keys = log_likelihood(bigrams, negative_reviews)[:150]\n",
    "neutral_keys = log_likelihood(bigrams, neutral_reviews)[:150]\n",
    "\n",
    "# serialize keywords, so you don't have t\n",
    "with open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/positive_keys.pkl\", \"wb\") as outfile:\n",
    "  pickle.dump(positive_keys, outfile)\n",
    "\n",
    "with open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/negative_keys.pkl\", \"wb\") as outfile:\n",
    "  pickle.dump(negative_keys, outfile)\n",
    "\n",
    "with open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/neutral_keys.pkl\", \"wb\") as outfile:\n",
    "  pickle.dump(neutral_keys, outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdV1RcVmD020"
   },
   "source": [
    "The three list contain tuples of bigram, log likelihood, overall frequency and frequency in the subcorpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29Dfj5hlk1e5"
   },
   "source": [
    "# Interpret your results\n",
    "## Keyness vs frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e20ESdSnprmk"
   },
   "outputs": [],
   "source": [
    "# only needed if we have to reconnect\n",
    "import pickle\n",
    "positive_keys = pickle.load(open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/positive_keys.pkl\", \"rb\"))\n",
    "negative_keys = pickle.load(open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/negative_keys.pkl\", \"rb\"))\n",
    "neutral_keys = pickle.load(open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/neutral_keys.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fx5SBr5NLypH"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/data/processed/positive_keys.tsv\", \"w\") as outfile:\n",
    "    h = \"bigram\\tloglikelihood\\tcorpus_freq\\treference_freq\\n\"\n",
    "    outfile.write(h)\n",
    "    for e in positive_keys:\n",
    "        wd, ll, cf, rf = e[0], str(e[1]), str(e[2]), str(e[3])\n",
    "        o = \"\\t\".join([wd, ll, cf, rf]) + \"\\n\"\n",
    "        outfile.write(o)\n",
    "\n",
    "with open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/data/processed/negative_keys.tsv\", \"w\") as outfile:\n",
    "    h = \"bigram\\tloglikelihood\\tcorpus_freq\\treference_freq\\n\"\n",
    "    outfile.write(h)\n",
    "    for e in negative_keys:\n",
    "        wd, ll, cf, rf = e[0], str(e[1]), str(e[2]), str(e[3])\n",
    "        o = \"\\t\".join([wd, ll, cf, rf]) + \"\\n\"\n",
    "        outfile.write(o)\n",
    "\n",
    "with open(\"/content/drive/My Drive/crowintelligence/projektek/manning/sentiment_analysis_project/Colab/data/processed/neutral_keys.tsv\", \"w\") as outfile:\n",
    "    h = \"bigram\\tloglikelihood\\tcorpus_freq\\treference_freq\\n\"\n",
    "    outfile.write(h)\n",
    "    for e in neutral_keys:\n",
    "        wd, ll, cf, rf = e[0], str(e[1]), str(e[2]), str(e[3])\n",
    "        o = \"\\t\".join([wd, ll, cf, rf]) + \"\\n\"\n",
    "        outfile.write(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lkkYPPGr66iM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "positive_df = pd.DataFrame({\"bigrams\": [e[0] for e in positive_keys],\n",
    "                     \"keyness\": [e[1] for e in positive_keys],\n",
    "                     \"corpus_freq\": [e[2] for e in positive_keys],\n",
    "                     \"reference_freq\": [e[3] for e in positive_keys]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 756,
     "status": "ok",
     "timestamp": 1587903419282,
     "user": {
      "displayName": "Zoltán Varjú",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_2m2F3Q5ufS6p0I4a7oD7NgOzPTaq7hFklmEinQ=s64",
      "userId": "15747406381911019387"
     },
     "user_tz": -120
    },
    "id": "TcSvgysTk7A0",
    "outputId": "be21dc56-16d9-48fd-d5c8-b180dda1fdf4"
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.Chart(positive_df).mark_point().encode(\n",
    "    x='keyness:Q',\n",
    "    y='reference_freq:Q',\n",
    "    color='reference_freq:Q',\n",
    "    tooltip=[\"bigrams:N\", \"reference_freq:Q\"],\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 906,
     "status": "ok",
     "timestamp": 1587903602468,
     "user": {
      "displayName": "Zoltán Varjú",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_2m2F3Q5ufS6p0I4a7oD7NgOzPTaq7hFklmEinQ=s64",
      "userId": "15747406381911019387"
     },
     "user_tz": -120
    },
    "id": "FIkXl_s5K6lI",
    "outputId": "0297ec89-c1ad-4653-ba42-00898d49231c"
   },
   "outputs": [],
   "source": [
    "negative_df = pd.DataFrame({\"bigrams\": [e[0] for e in negative_keys],\n",
    "                     \"keyness\": [e[1] for e in negative_keys],\n",
    "                     \"corpus_freq\": [e[2] for e in negative_keys],\n",
    "                     \"reference_freq\": [e[3] for e in negative_keys]})\n",
    "\n",
    "alt.Chart(negative_df).mark_point().encode(\n",
    "    x='keyness:Q',\n",
    "    y='reference_freq:Q',\n",
    "    color='keyness:Q',\n",
    "    tooltip=[\"words:N\", \"reference_freq:Q\"],\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1044,
     "status": "ok",
     "timestamp": 1587839930015,
     "user": {
      "displayName": "Zoltán Varjú",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_2m2F3Q5ufS6p0I4a7oD7NgOzPTaq7hFklmEinQ=s64",
      "userId": "15747406381911019387"
     },
     "user_tz": -120
    },
    "id": "IwSyDkRlNoZL",
    "outputId": "eb3faac5-620b-41fd-b5f5-bb574608f740"
   },
   "outputs": [],
   "source": [
    "neutral_df = pd.DataFrame({\"bigrams\": [e[0] for e in neutral_keys],\n",
    "                     \"keyness\": [e[1] for e in neutral_keys],\n",
    "                     \"corpus_freq\": [e[2] for e in neutral_keys],\n",
    "                     \"reference_freq\": [e[3] for e in neutral_keys]})\n",
    "\n",
    "alt.Chart(neutral_df).mark_point().encode(\n",
    "    x='keyness:Q',\n",
    "    y='reference_freq:Q',\n",
    "    color='keyness:Q',\n",
    "    tooltip=[\"words:N\", \"reference_freq:Q\"],\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MVg6YcGLp4qB"
   },
   "source": [
    "## See the context\n",
    "First, we have to make nltk corpora from the subcorpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIBIwBhyp9Mp"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "positive_texts = [reviews[i] for i in range(len(reviews)) if predicted_class[i] == 2]\n",
    "negative_texts = [reviews[i] for i in range(len(reviews)) if predicted_class[i] == 0]\n",
    "neutral_texts = [reviews[i] for i in range(len(reviews)) if predicted_class[i] == 1]\n",
    "\n",
    "positive_texts = \"\\n\".join(positive_texts)\n",
    "negative_texts = \"\\n\".join(negative_texts)\n",
    "neutral_texts = \"\\n\".join(neutral_texts)\n",
    "\n",
    "def make_text_corpus(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return nltk.Text(tokens)\n",
    "\n",
    "\n",
    "positive_text = make_text_corpus(positive_texts)\n",
    "negative_text = make_text_corpus(negative_texts)\n",
    "neutral_text = make_text_corpus(neutral_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYlJf6cEqyvO"
   },
   "source": [
    "Now, we can check the context of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tiatv62dsKL0"
   },
   "outputs": [],
   "source": [
    "words_to_check = [\n",
    "    \"word\",\n",
    "    \"great\",\n",
    "    \"game\",\n",
    "    \"love\",\n",
    "    \"works\",\n",
    "    \"highly\",\n",
    "    \"recommend\",\n",
    "    \"best\",\n",
    "    \"product\",\n",
    "    \"awesome\",\n",
    "    \"best\",\n",
    "    \"ever\",\n",
    "    \"great\",\n",
    "    \"excellent\"\n",
    "]\n",
    "\n",
    "for wd in words_to_check:\n",
    "    positive_text.concordance(wd)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMUgrG8mAUTLmbHQDhrQaLY",
   "collapsed_sections": [],
   "name": "final_report.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "growth_hack:Python",
   "language": "python",
   "name": "conda-env-growth_hack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
